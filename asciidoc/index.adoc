= Distributed Job Manager
:toc: left
:toclevels: 4
:source-highlighter: coderay

[abstract]
DistributedJobManager will schedule tasks and balance workload in the cluster.
[link=https://search.maven.org/search?q=g:ru.fix%20and%20a:distributed-job-manager]
image::https://img.shields.io/maven-central/v/ru.fix/distributed-job-manager.svg[]
image:https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png[32,32]
link:https://github.com/ru-fix/distributed-job-manager[]

== Concept

DistributedJobManager (DJM) stores it's state in link:https://zookeeper.apache.org/[ZooKeeper] +
DJM lives withing JVM application and maintain it's own thread pool. +
DJM regularly launches user defined jobs in separate threads based on schedule. +
User defined Job is any class that implements `DistributedJob` interface. +
DJM will restart Job in case of any failure. +
DJM balance workload between Jobs +
DJM consists of two parts: Manager and Worker. Each DJM instance has active Worker.
  But only one DJM instance has active Manager. +
Manager orchestrate Job between Workers in the cluster.

image::djm-zk.png[]

User defined Job should implement `DistributedJob` interface and provide information about work-items. +
Work-item is a smallest indivisible peace of work. +
Job WorkPool is a list of work-items that DJM will split between active Workers.
[code]
----
class MyJob implements DistributedJob{
    //...
    WorkPool getWorkPool(){
        return WorkPool.of(new HashSet<>(Arrays.asList("workItem1", "workItem2")));
    }
}
----

Job informs DJM about WorkPool. +
DJM split work-items from WorkPool among all Workers according to assignment strategy. +
If Job define WorkPool with single work-item then it means that such job will be launched by DJM
only within single application in the cluster.

When DJM launches Job it passes information about work-items that Job should process.
[code]
----
class MyJob implements DistributedJob{
    //...
void run(DistributedJobContext context) {
    //...
    Set<String> workShare = context.getWorkShare();
    for (String workItem : workShare) {
        //process workItem
    }
}
----

include::assignment-strategies.adoc[]

=== Example of DJM configuration
:sourcedir: ../distributed-job-manager/src/test/kotlin

[source,kotlin]
----
include::{sourcedir}/ru/fix/distributed/job/manager/example/DjmConfigurationExample.kt[]
----

image::djm.png[]


== Getting started

=== Add dependency into your project.

.Java dependencies
* distributed-job-manager image:https://img.shields.io/maven-central/v/ru.fix/distributed-job-manager.svg[link=https://search.maven.org/search?q=g:ru.fix%20and%20a:distributed-job-manager]

=== Launch DJM
DJM requires list of ZooKeeper hosts and path within ZooKeeper that will be used to store DJM state. +
DJM will use provided nodeId to identify application instance within cluster. +

[code]
----
//During application startup
DistributedJobManager djm = new DistributedJobManager(
    "zooKeeperHost1,zooKeeperHost2,zooKeeperHost3"
    "/zooKeeperPath/for/djm",
    "applicaiotId#3",
    Arrays.asList(new MyJob1(), new MyJob2()));

//During applicatoin shutdown
djm.close();
----

=== Monitoring and metrics
DJM expose Jobs state through profiler metrics. +
* What tasks are currently running.
* How long it took for task to complete.
* How many work items processed.
* etc.


== Reassignment
Reassignment is a process when Master change Job and work-items assigment between application instances in the cluster. +
Reassignment is triggered by

* One of nodes shutdown or restart
* Network disconnect
* Job WorkPool changes

Rebalance steps:

image::djm-reassignment.png[]

* Manager reads from zookeeper availabiliy state - list of jobs, where they can be launched and list of each job work-items
* Manager reads from zookeeper assignment state - on which nodes jobs and work-items are launched or scheduled right now.
* Manager calculate new assignment state
* Manager writes new assignment state to zookeeper
* Workers receive notification from zookeeper about assignment state update
* Workers stops old Jobs and launches new Jobs according to new assignment state.

Main goal of rebalance process is to minimize unnecessary job restarts and reassignments.

== State
DJM keeps cluster state as a tree of zookeeper nodes. +
Once created, DJM initializes zookeeper paths, if needed:
```
job-manager
  └ alive
  └ locks
  └ leader-latch
  └ workers
  └ work-pool
  └ work-pool-version
  └ worker-assignment-version
  └ worker-version
```
Worker life-cycle: +

Every time worker (re)connected to cluster, it process few step in single transaction,
using `work-pool-version` and `worker-version` locks: +

* (re)creates ephemeral node in `alive` subtree, to inform manager about (re)connecting
```
  └ alive
    └ worker_1
```
* (re)creates its subtree with empty `assigned` subtree (this needs `worker-version` lock)
```
  └ workers
    └ worker_1
      └ available
        └ async.report.building.job
        └ elasticsearch.upload.job
      └ assigned
```
* registers (or updates) its jobs in common `work-pool` subtree (this needs  `work-pool-version` lock)
```
  └ work-pool
    └ async.report.building.job
      └ workItemA
      └ workItemB
    └ elasticsearch.upload.job
      └ workItemC
```

Worker listens for updates in `workers/worker/assigned` subtree and stops/launches jobs accordingly. +
Worker also periodically updates its jobs in `work-pool` subtree using `work-pool-version` lock +

Manager life-cycle:

Manager listens for updates in `alive` subtree and invokes rebalance if set of alive workers was changed: +

* Updates workers `assigned` subtree using `worker-assignment-version` lock

In future releases:

* Manager also listens for updates in `work-pool` subtree (dynamic work-pool)
* Manager also removes not relevant (which isn't in any `workers/*/available` subtree) jobs from `work-pool` subtree
using `work-pool-version` lock (clearing process)

Full ZK tree you can see below:
```
job-manager
  └ alive //alive workers that can run jobs
    └ 20 //worker with id 20
    └ 3  //worker with id 3
    ...
  └ locks //locks guard work-items: ony one Job can access work-item in the same time
    └ async.report.building.job //job id
      └ workItemA.lock  //list of job locks so only one node could run job with same work-item
    └ elasticsearch.upload.job
      └ workItemC.lock
      ...
  └ leader-latch // used for Manager election
    └ ...
  └ workers //list of workers
    └ 20
     ...
    └ 3
      └ available //list of jobs that worker with id `3` can run
        └ async.report.building.job
        └ elasticsearch.upload.job
         ...
      └ assigned  //List of assigned jobs to worker '3'
        └ async.report.building.job
          └ workItemA  //report.building job with `workItemA` and `workItemB` is assigned to worker `3`
          └ workItemB
        └ elasticsearch.upload.job
          └ workItemC
         ...
  └ work-pool //common work-pool for all jobs
    └ async.report.building.job
      └ workItemA //Work pool of report.building Job
      └ workItemB
    └ elasticsearch.upload.job
      └ workItemC
       ...
  └ work-pool-version //part of transaction, allows atomicaly update available work-pool
  └ worker-assignment-version //part of transaction, allows atomicaly assign jobs
  └ worker-version //part of transaction, allows atomicaly register workier
```

== Guarantees

In case of network failure one one can be temporary detached from other part of the cluster. +
In order to improve stability of the cluster and tolerate short connectivity problems
DJM allows detached Worker to continue it's work for configured amount of time. +
To enable that DJM uses persistent locks and keep information when which worker started to process particular job in ZooKeeper. +
Other Workers will not be able to process same work-items during this lock timeout even if owner of lock gone offline.

image::djm-zk-disconnect.png[]


include::source-guidebook.adoc[]
