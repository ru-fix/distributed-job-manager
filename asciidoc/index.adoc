= Distributed Job Manager
:toc: left
:toclevels: 4
:source-highlighter: coderay

[abstract]
DistributedJobManager will schedule tasks and balance workload in the cluster.
[link=https://search.maven.org/search?q=g:ru.fix%20and%20a:distributed-job-manager]
image::https://img.shields.io/maven-central/v/ru.fix/distributed-job-manager.svg[]
image:https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png[32,32]
link:https://github.com/ru-fix/distributed-job-manager[]

== Concept

DistributedJobManager (DJM) stores it's state in link:https://zookeeper.apache.org/[ZooKeeper] +
DJM lives withing JVM application and maintain it's own thread pool. +
DJM regularly launches user defined jobs in separate threads based on schedule. +
User defined Job is any class that implements `DistributedJob` interface. +
DJM will restart Job in case of any failure. +
DJM balance workload between Jobs +
DJM consists of two parts: Manager and Worker. Each DJM instance has active Worker.
  But only one DJM instance has active Manager. +
Manager orchestrate Job between Workers in the cluster.

image::djm-zk.png[]

User defined Job should implement `DistributedJob` interface and provide information about work-items. +
Work-item is a smallest indivisible peace of work. +
Job WorkPool is a list of work-items that DJM will split between active Workers.
[code]
----
class MyJob implements DistributedJob{
    //...
    WorkPool getWorkPool(){
        return WorkPool.of(new HashSet<>(Arrays.asList("workItem1", "workItem2")));
    }
}
----

Job informs DJM about WorkPool. +
DJM split work-items from WorkPool among all Workers according to assignment strategy. +
If Job define WorkPool with single work-item then it means that such job will be launched by DJM
only within single application in the cluster.

When DJM launches Job it passes information about work-items that Job should process.
[code]
----
class MyJob implements DistributedJob{
    //...
void run(DistributedJobContext context) {
    //...
    Set<String> workShare = context.getWorkShare();
    for (String workItem : workShare) {
        //process workItem
    }
}
----

include::assignment-strategies.adoc[]

=== Example of DJM configuration
:sourcedir: ../distributed-job-manager/src/test/kotlin

[source,kotlin]
----
include::{sourcedir}/ru/fix/distributed/job/manager/example/DjmConfigurationExample.kt[]
----

image::djm.png[]


== Getting started

=== Add dependency into your project.

.Java dependencies
* distributed-job-manager image:https://img.shields.io/maven-central/v/ru.fix/distributed-job-manager.svg[link=https://search.maven.org/search?q=g:ru.fix%20and%20a:distributed-job-manager]

=== Launch DJM
DJM requires list of ZooKeeper hosts and path within ZooKeeper that will be used to store DJM state. +
DJM will use provided nodeId to identify application instance within cluster. +

[code]
----
//During application startup
DistributedJobManager djm = new DistributedJobManager(
    "zooKeeperHost1,zooKeeperHost2,zooKeeperHost3"
    "/zooKeeperPath/for/djm",
    "applicaiotId#3",
    Arrays.asList(new MyJob1(), new MyJob2()));

//During applicatoin shutdown
djm.close();
----

=== Monitoring and metrics
DJM expose Jobs state through profiler metrics. +
* What tasks are currently running.
* How long it took for task to complete.
* How many work items processed.
* etc.


== Reassignment
Reassignment is a process when Master change Job and work-items assigment between application instances in the cluster. +
Reassignment is triggered by

* One of nodes shutdown or restart
* Network disconnect
* Job WorkPool changes

Rebalance steps:

image::djm-reassignment.png[]

* Manager reads from zookeeper availabiliy state - list of jobs, where they can be launched and list of each job work-items
* Manager reads from zookeeper assignment state - on which nodes jobs and work-items are launched or scheduled right now.
* Manager calculate new assignment state
* Manager writes new assignment state to zookeeper
* Workers receive notification from zookeeper about assignment state update
* Workers stops old Jobs and launches new Jobs according to new assignment state.

Main goal of rebalance process is to minimize unnecessary job restarts and reassignments.

== State
DJM keeps cluster state as a tree of zookeeper nodes. +
Worker registers jobs which it can run and updates jobs work-items in common work-pool +
Manager updates 'assigned' subtree and removes not relevant jobs from common work-pool +
Worker listens for updates in 'assigned' subtree and stop/launch jobs accordingly. +
Manager listens for updates in 'alive' subtree and 'work-pool' subtree.
These updates not trigger rebalance directly. Manager puts them in queue, and checks this queue regularly in separate thread.


```
job-manager
  └ alive //alive workers that can run jobs
    └ 20 //worker with id 20
    └ 3  //worker with id 3
    ...
  └ locks //locks guard work-items: ony one Job can access work-item in the same time
    └ async.report.building.job //job id
      └ workItemA.lock  //list of job locks so only one node could run job with same work-item
    └ elasticsearch.upload.job
      └ workItemC.lock
      ...
  └ assignment-version //part of transaction, allows atomicaly assign jobs
  └ registration-version //part of transaction, allows atomicaly register workier
  └ leader-latch // used for Manager election
    └ ...
  └ workers //list of workers
    └ 20
     ...
    └ 3
      └ available //list of jobs that worker with id `3` can run
        └ async.report.building.job
        └ elasticsearch.upload.job
         ...
      └ assigned                 //List of assigned jobs to worker '3'
        └ async.report.building.job
          └ workItemA  //report.building job with `workItemA` and `workItemB` is assigned to worker `3`
          └ workItemB
        └ elasticsearch.upload.job
          └ workItemC
         ...
  └ work-pool //common work-pool for all jobs
    └ async.report.building.job
      └ workItemA //Work pool of report.building Job
      └ workItemB
    └ elasticsearch.upload.job
      └ workItemC
       ...
```

== Guarantees

In case of network failure one one can be temporary detached from other part of the cluster. +
In order to improve stability of the cluster and tolerate short connectivity problems
DJM allows detached Worker to continue it's work for configured amount of time. +
To enable that DJM uses persistent locks and keep information when which worker started to process particular job in ZooKeeper. +
Other Workers will not be able to process same work-items during this lock timeout even if owner of lock gone offline.

image::djm-zk-disconnect.png[]


include::source-guidebook.adoc[]
